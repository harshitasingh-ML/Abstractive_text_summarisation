{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Abstractive_Text_summary.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zL9xqkv8q6e4"
      },
      "source": [
        "# Asbtractive Text Summarization:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWNU6ArPq6Y7"
      },
      "source": [
        "## News Summary Data from inshorts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRDOV1Hr3BoM"
      },
      "source": [
        "#!pip install chart_studio"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCB9cqoRtNNX"
      },
      "source": [
        "#!gdown https://drive.google.com/uc?id=1apcfE6-zkIQfXJbJNx2cLQVzpfSXOH7m"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fU2zU7bM5K6K"
      },
      "source": [
        "#!gdown https://drive.google.com/uc?id=1gyjAv5UiY5KeJAEsjhm6g5wUZkaRVFbl"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzwqA2qX5Ku4"
      },
      "source": [
        "#!gdown https://drive.google.com/uc?id=1QGZ8_P0TwqgFNE8SvxnMoIUNIiI-z6ci"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKBYdYedqfVG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88cef7a5-802c-40c4-e244-e0c312417699"
      },
      "source": [
        "import gc\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import collections\n",
        "import pickle\n",
        "import numpy as np\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from gensim.test.utils import get_tmpfile\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "\n",
        "import os\n",
        "import chart_studio.plotly.plotly as py\n",
        "import plotly.graph_objs as go\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.feature_extraction import stop_words\n",
        "import re\n",
        "import string\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os, sys, tarfile\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from IPython.display import Image\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, TimeDistributed, Layer\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning:\n",
            "\n",
            "The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1AiJo-37mO6"
      },
      "source": [
        "pd.set_option('display.max_info_columns',1000)\n",
        "pd.set_option('display.max_colwidth',5000)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "HSrc2r7hUlh_",
        "outputId": "44d221a3-9552-4331-a7b8-9e4860736e5e"
      },
      "source": [
        "news1 = pd.read_csv('/content/news_summary.csv', encoding='latin-1', usecols=['headlines', 'text'])\n",
        "news2 = pd.read_csv('/content/news_summary_more.csv', encoding='latin-1')\n",
        "news_summary = pd.concat([news1, news2], axis=0).reset_index(drop=True)\n",
        "news_summary.head()       "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headlines</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Daman &amp; Diu revokes mandatory Rakshabandhan in offices order</td>\n",
              "      <td>The Administration of Union Territory Daman and Diu has revoked its order that made it compulsory for women to tie rakhis to their male colleagues on the occasion of Rakshabandhan on August 7. The administration was forced to withdraw the decision within 24 hours of issuing the circular after it received flak from employees and was slammed on social media.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Malaika slams user who trolled her for 'divorcing rich man'</td>\n",
              "      <td>Malaika Arora slammed an Instagram user who trolled her for \"divorcing a rich man\" and \"having fun with the alimony\". \"Her life now is all about wearing short clothes, going to gym or salon, enjoying vacation[s],\" the user commented. Malaika responded, \"You certainly got to get your damn facts right before spewing sh*t on me...when you know nothing about me.\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>'Virgin' now corrected to 'Unmarried' in IGIMS' form</td>\n",
              "      <td>The Indira Gandhi Institute of Medical Sciences (IGIMS) in Patna on Thursday made corrections in its Marital Declaration Form by changing 'Virgin' option to 'Unmarried'. Earlier, Bihar Health Minister defined virgin as being an unmarried woman and did not consider the term objectionable. The institute, however, faced strong backlash for asking new recruits to declare their virginity in the form.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Aaj aapne pakad liya: LeT man Dujana before being killed</td>\n",
              "      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana, who was killed by security forces, said \"Kabhi hum aage, kabhi aap, aaj aapne pakad liya, mubarak ho aapko (Today you caught me. Congratulations)\" after being caught. He added that he won't surrender, and whatever is in his fate will happen to him. \"Hum nikley they shaheed hone (had left home for martyrdom),\" he added.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hotel staff to get training to spot signs of sex trafficking</td>\n",
              "      <td>Hotels in Maharashtra will train their staff to spot signs of sex trafficking, including frequent requests for bed linen changes and 'Do not disturb' signs left on room doors for days. A mobile phone app called Rescue Me, which will allow staff to alert police of suspicious behaviour, will be developed. The initiative has been backed by the Maharashtra government.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                      headlines                                                                                                                                                                                                                                                                                                                                                                                                            text\n",
              "0  Daman & Diu revokes mandatory Rakshabandhan in offices order                                          The Administration of Union Territory Daman and Diu has revoked its order that made it compulsory for women to tie rakhis to their male colleagues on the occasion of Rakshabandhan on August 7. The administration was forced to withdraw the decision within 24 hours of issuing the circular after it received flak from employees and was slammed on social media.\n",
              "1  Malaika slams user who trolled her for 'divorcing rich man'                                        Malaika Arora slammed an Instagram user who trolled her for \"divorcing a rich man\" and \"having fun with the alimony\". \"Her life now is all about wearing short clothes, going to gym or salon, enjoying vacation[s],\" the user commented. Malaika responded, \"You certainly got to get your damn facts right before spewing sh*t on me...when you know nothing about me.\"\n",
              "2          'Virgin' now corrected to 'Unmarried' in IGIMS' form  The Indira Gandhi Institute of Medical Sciences (IGIMS) in Patna on Thursday made corrections in its Marital Declaration Form by changing 'Virgin' option to 'Unmarried'. Earlier, Bihar Health Minister defined virgin as being an unmarried woman and did not consider the term objectionable. The institute, however, faced strong backlash for asking new recruits to declare their virginity in the form.\n",
              "3      Aaj aapne pakad liya: LeT man Dujana before being killed                                Lashkar-e-Taiba's Kashmir commander Abu Dujana, who was killed by security forces, said \"Kabhi hum aage, kabhi aap, aaj aapne pakad liya, mubarak ho aapko (Today you caught me. Congratulations)\" after being caught. He added that he won't surrender, and whatever is in his fate will happen to him. \"Hum nikley they shaheed hone (had left home for martyrdom),\" he added.\n",
              "4  Hotel staff to get training to spot signs of sex trafficking                                  Hotels in Maharashtra will train their staff to spot signs of sex trafficking, including frequent requests for bed linen changes and 'Do not disturb' signs left on room doors for days. A mobile phone app called Rescue Me, which will allow staff to alert police of suspicious behaviour, will be developed. The initiative has been backed by the Maharashtra government."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RipuwyhEYyIe",
        "outputId": "a3f8f6f9-16b0-4688-a3bf-e1b05618420c"
      },
      "source": [
        "news_summary.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(102915, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeYa6zG878pI"
      },
      "source": [
        "## Text Preprocessing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRgTDYJNUlVX"
      },
      "source": [
        "def preprocess(text):\n",
        "    \"\"\"Preprocess the given text.\"\"\"\n",
        "    \n",
        "    # Encode to ascii\n",
        "    #text = ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')\n",
        "    \n",
        "    # To lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    text = re.sub(\"(\\\\t)\", ' ', text)  #remove escape charecters\n",
        "    text = re.sub(\"(\\\\r)\", ' ', text)\n",
        "    text = re.sub(\"(\\\\n)\", ' ', text)\n",
        "    text = re.sub(\"(__+)\", ' ', text)   #remove _ if it occors more than one time consecutively\n",
        "    text = re.sub(\"(--+)\", ' ', text)   #remove - if it occors more than one time consecutively\n",
        "    text = re.sub(\"(~~+)\", ' ', text)   #remove ~ if it occors more than one time consecutively\n",
        "    text = re.sub(\"(\\+\\++)\", ' ', text)   #remove + if it occors more than one time consecutively\n",
        "    text = re.sub(\"(\\.\\.+)\", ' ', text)   #remove . if it occors more than one time consecutively\n",
        "    text = re.sub(r\"[<>()|&©ø\\[\\]\\'\\\",;?~*!]\", ' ', text) #remove <>()|&©ø\"',;?~*!\n",
        "    text = re.sub(\"(mailto:)\", ' ', text)  #remove mailto:\n",
        "    text = re.sub(r\"(\\\\x9\\d)\", ' ', text)  #remove \\x9* in text\n",
        "    text = re.sub(\"([iI][nN][cC]\\d+)\", 'INC_NUM', text)  #replace INC nums to INC_NUM\n",
        "    text = re.sub(\"([cC][mM]\\d+)|([cC][hH][gG]\\d+)\", 'CM_NUM', text)  #replace CM# and CHG# to CM_NUM\n",
        "    text = re.sub(\"(\\.\\s+)\", ' ', text)  #remove full stop at end of words(not between)\n",
        "    text = re.sub(\"(\\-\\s+)\", ' ', text)  #remove - at end of words(not between)\n",
        "    text = re.sub(\"(\\:\\s+)\", ' ', text)  #remove : at end of words(not between)\n",
        "    text = re.sub(\"(\\s+.\\s+)\", ' ', text)  #remove any single charecters hanging between 2 spaces\n",
        "\n",
        "    #Replace any url as such https://abc.xyz.net/browse/sdf-5327 ====> abc.xyz.net\n",
        "    try:\n",
        "        url = re.search(r'(https*:\\/*)([^\\/\\s]+)(.[^\\s]+)', text)\n",
        "        repl_url = url.group(2)\n",
        "        text = re.sub(r'(https*:\\/*)([^\\/\\s]+)(.[^\\s]+)',repl_url, text)\n",
        "    except:\n",
        "        pass #there might be emails with no url in them\n",
        "\n",
        "    text = re.sub(\"(\\s+)\",' ',text) #remove multiple spaces\n",
        "    text = re.sub(\"(\\s+.\\s+)\", ' ', text) #remove any single charecters hanging between 2 spaces\n",
        "    return text\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "GJ1mXITecdvB",
        "outputId": "7b05f060-e36f-48fb-ebdf-5932b238d8ca"
      },
      "source": [
        "news_summary['headlines'] = news_summary['headlines'].apply(preprocess)\n",
        "news_summary['text'] = news_summary['text'].apply(preprocess)\n",
        "news_summary.head(1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headlines</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>daman diu revokes mandatory rakshabandhan in offices order</td>\n",
              "      <td>the administration of union territory daman and diu has revoked its order that made it compulsory for women to tie rakhis to their male colleagues on the occasion of rakshabandhan on august the administration was forced to withdraw the decision within 24 hours of issuing the circular after it received flak from employees and was slammed on social media.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    headlines                                                                                                                                                                                                                                                                                                                                                                 text\n",
              "0  daman diu revokes mandatory rakshabandhan in offices order  the administration of union territory daman and diu has revoked its order that made it compulsory for women to tie rakhis to their male colleagues on the occasion of rakshabandhan on august the administration was forced to withdraw the decision within 24 hours of issuing the circular after it received flak from employees and was slammed on social media."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "xwuoJeSEcdrr",
        "outputId": "3aeff1d9-a17a-4665-fbf3-573d0c58343d"
      },
      "source": [
        "text_len = news_summary['text'].str.split().apply(len)\n",
        "headline_len = news_summary['headlines'].str.split().apply(len)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(headline_len)\n",
        "plt.title('Headlines length distribution')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(text_len)\n",
        "plt.title('Text length distribution')\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUFtbWBQcdpN",
        "outputId": "fb50eeb8-6f3f-4cc0-da82-46b4078ebba5"
      },
      "source": [
        "text_len.mean(),headline_len.mean()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(56.92494777243356, 9.438177136471845)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcQ_o_T9hf-g",
        "outputId": "16a15cdd-1fca-4b7d-d9c3-10734ee1ea39"
      },
      "source": [
        "print(f\"Headlines having length in range [0, 15]: {len(headline_len[headline_len <= 15])/len(headline_len)}\")\n",
        "\n",
        "# Check how much % of text have 0-62 words\n",
        "print(f\"Text having length in range [0, 62]: {len(text_len[text_len <= 63])/len(text_len)}\")\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Headlines having length in range [0, 15]: 0.9999805664869067\n",
            "Text having length in range [0, 62]: 0.9992906767720935\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkO01kA3hSjS"
      },
      "source": [
        "MAX_TEXT_SEQ_LEN = 63   # 63 covers 99.99% of the data\n",
        "MAX_HEADLINE_SEQ_LEN = 15  # 15 covers 99.99% of the data\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxMkRqqDlDem"
      },
      "source": [
        "START_TOKEN = '<start> '\n",
        "END_TOKEN = ' <end>'\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "id": "IpKWJqM-hSf6",
        "outputId": "04aa5c6b-f1a2-4c7d-c683-b035d09ea5a6"
      },
      "source": [
        "news_summary['headlines_input'] = START_TOKEN + news_summary['headlines']\n",
        "news_summary['headlines_output'] = news_summary['headlines'] + END_TOKEN\n",
        "news_summary = news_summary.drop(['headlines'], axis=1)\n",
        "news_summary.head(2)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>headlines_input</th>\n",
              "      <th>headlines_output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>the administration of union territory daman and diu has revoked its order that made it compulsory for women to tie rakhis to their male colleagues on the occasion of rakshabandhan on august the administration was forced to withdraw the decision within 24 hours of issuing the circular after it received flak from employees and was slammed on social media.</td>\n",
              "      <td>&lt;start&gt; daman diu revokes mandatory rakshabandhan in offices order</td>\n",
              "      <td>daman diu revokes mandatory rakshabandhan in offices order &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>malaika arora slammed an instagram user who trolled her for divorcing rich man and having fun with the alimony her life now is all about wearing short clothes going to gym or salon enjoying vacation the user commented malaika responded you certainly got to get your damn facts right before spewing sh on me when you know nothing about me</td>\n",
              "      <td>&lt;start&gt; malaika slams user who trolled her for divorcing rich man</td>\n",
              "      <td>malaika slams user who trolled her for divorcing rich man  &lt;end&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                  text  ...                                                  headlines_output\n",
              "0  the administration of union territory daman and diu has revoked its order that made it compulsory for women to tie rakhis to their male colleagues on the occasion of rakshabandhan on august the administration was forced to withdraw the decision within 24 hours of issuing the circular after it received flak from employees and was slammed on social media.  ...  daman diu revokes mandatory rakshabandhan in offices order <end>\n",
              "1                   malaika arora slammed an instagram user who trolled her for divorcing rich man and having fun with the alimony her life now is all about wearing short clothes going to gym or salon enjoying vacation the user commented malaika responded you certainly got to get your damn facts right before spewing sh on me when you know nothing about me   ...  malaika slams user who trolled her for divorcing rich man  <end>\n",
              "\n",
              "[2 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uIgtYkOhSc-",
        "outputId": "a311c6ca-afd5-41b6-e773-d7014727ee98"
      },
      "source": [
        "X_train, X_test = train_test_split(news_summary, test_size=0.1)\n",
        "\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(92623, 3)\n",
            "(10292, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRpWQqeKhSbL"
      },
      "source": [
        "# !\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\n",
        "\n",
        "def data_preparation(X_train, X_test):\n",
        "    \"\"\"Tokenize and pad the given text.\"\"\"\n",
        "    \n",
        "    # Fit tokenizers\n",
        "    text_tokenizer = Tokenizer()\n",
        "    text_tokenizer.fit_on_texts(X_train['text'])\n",
        "\n",
        "    headline_tokenizer = Tokenizer()\n",
        "    headline_tokenizer.fit_on_texts(X_train['headlines_input'])\n",
        "    \n",
        "    # Pad sequences\n",
        "    text_train = pad_sequences(text_tokenizer.texts_to_sequences(X_train['text']), maxlen=MAX_TEXT_SEQ_LEN, padding='post', truncating='post')\n",
        "    text_test = pad_sequences(text_tokenizer.texts_to_sequences(X_test['text']), maxlen=MAX_TEXT_SEQ_LEN, padding='post', truncating='post')\n",
        "\n",
        "    headline_train_input = pad_sequences(headline_tokenizer.texts_to_sequences(X_train['headlines_input']), maxlen=MAX_HEADLINE_SEQ_LEN, padding='post', truncating='post')\n",
        "    headline_train_output = pad_sequences(headline_tokenizer.texts_to_sequences(X_train['headlines_output']), maxlen=MAX_HEADLINE_SEQ_LEN, padding='post', truncating='post')\n",
        "    headline_test_input = pad_sequences(headline_tokenizer.texts_to_sequences(X_test['headlines_input']), maxlen=MAX_HEADLINE_SEQ_LEN, padding='post', truncating='post')\n",
        "    headline_test_output = pad_sequences(headline_tokenizer.texts_to_sequences(X_test['headlines_output']), maxlen=MAX_HEADLINE_SEQ_LEN, padding='post', truncating='post')\n",
        "\n",
        "    return {\n",
        "        'text_tokenizer': text_tokenizer,\n",
        "        'headline_tokenizer': headline_tokenizer,\n",
        "        'text_train': text_train,\n",
        "        'text_test': text_test,\n",
        "        'headline_train_input': headline_train_input,\n",
        "        'headline_train_output': headline_train_output,\n",
        "        'headline_test_input': headline_test_input,\n",
        "        'headline_test_output': headline_test_output\n",
        "    }\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIDVcxwphSYm"
      },
      "source": [
        "preprep_data = data_preparation(X_train, X_test)\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O47NQbfUpd6e",
        "outputId": "62c8c85d-d3cf-450d-b5cc-c6fa8666dd35"
      },
      "source": [
        "text_vocab_size = len(preprep_data['text_tokenizer'].word_index) + 1\n",
        "headline_vocab_size = len(preprep_data['headline_tokenizer'].word_index) + 1\n",
        "\n",
        "print('Text vocab size:{} ,Headline vocab size: {}'.format(text_vocab_size,headline_vocab_size))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text vocab size:79897 ,Headline vocab size: 34672\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0CUoZN87l9-",
        "outputId": "e53b7ff4-8595-44b8-9f77-3911522aa1ed"
      },
      "source": [
        "EM_DIM = 100\n",
        "embeddings_index = dict()\n",
        "f = open('/content/glove.6B.100d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1: ], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print(f'Found {len(embeddings_index)} word vectors.')\n",
        "\n",
        "headline_embedding_matrix = np.zeros((headline_vocab_size, EM_DIM))\n",
        "for word, i in preprep_data['headline_tokenizer'].word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        headline_embedding_matrix[i] = embedding_vector\n",
        "\n",
        "text_embedding_matrix = np.zeros((text_vocab_size, EM_DIM))\n",
        "for word, i in preprep_data['text_tokenizer'].word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        text_embedding_matrix[i] = embedding_vector\n",
        "\n",
        "\n",
        "print('Shape of headline embedding matrix:{},Shape of text embedding matrix:{}'. format(headline_embedding_matrix.shape,text_embedding_matrix.shape))\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n",
            "Shape of headline embedding matrix:(34672, 100),Shape of text embedding matrix:(79897, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-d0Dn37vP5X"
      },
      "source": [
        "## Model Building:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIQVg8dIpMBk"
      },
      "source": [
        "class Encoder(Layer):\n",
        "    def __init__(self, name):\n",
        "        super().__init__(name=name)\n",
        "        \n",
        "        self.embedding = Embedding(input_dim=text_vocab_size, output_dim=EM_DIM, weights=[text_embedding_matrix], input_length=MAX_TEXT_SEQ_LEN, trainable=False, name='encoder_embedding')\n",
        "        # Store encoder hidden state and cell state for decoder input. Hidden state is the output of last timestamp,\n",
        "        # which represents the entire input sequence using a single vector.\n",
        "        self.lstm = LSTM(units=128, return_sequences=True, return_state=True, name='encoder_lstm')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.embedding(x)\n",
        "        self.lstm_output, self.lstm_hidden, self.lstm_cell = self.lstm(x)\n",
        "        return self.lstm_output, self.lstm_hidden, self.lstm_cell\n",
        "    \n",
        "    def get_states(self):\n",
        "        return self.lstm_hidden, self.lstm_cell\n",
        "\n",
        "class Decoder(Layer):\n",
        "    def __init__(self, name):\n",
        "        super().__init__(name=name)\n",
        "        \n",
        "        self.embedding = Embedding(input_dim=headline_vocab_size, output_dim=EM_DIM, trainable=False, weights=[headline_embedding_matrix], input_length=None, name='decoder_embedding')\n",
        "        self.lstm = LSTM(units=128, return_sequences=True, return_state=True, name='decoder_lstm')\n",
        "    \n",
        "    \n",
        "    def call(self, x, lstm_hidden, lstm_cell):\n",
        "        x = self.embedding(x)\n",
        "        lstm_output, lstm_hidden, lstm_cell = self.lstm(x, initial_state=[lstm_hidden, lstm_cell])\n",
        "        return lstm_output, lstm_hidden, lstm_cell\n",
        "\n",
        "class EncoderDecoder(Model): \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(name='encoder')\n",
        "        self.decoder = Decoder(name='decoder')\n",
        "        self.decoder_dense = TimeDistributed(Dense(units=headline_vocab_size, activation='softmax'), name='decoder_dense')\n",
        "    \n",
        "    \n",
        "    def call(self, x):\n",
        "        text, summary = x\n",
        "        _, hidden_state, cell_state = self.encoder(text)\n",
        "        out, hidden_state, cell_state = self.decoder(summary, hidden_state, cell_state)\n",
        "        return self.decoder_dense(out)\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYHyrQeEpL_K"
      },
      "source": [
        "model_1 = EncoderDecoder()\n",
        "opt = tf.keras.optimizers.RMSprop(learning_rate=0.002)\n",
        "#opt = Adam(learning_rate=0.1)\n",
        "model_1.compile(optimizer=opt, loss='sparse_categorical_crossentropy')\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V93uoOzl-DXa"
      },
      "source": [
        "tf.keras.backend.clear_session() "
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6Jvq1JZpL80",
        "outputId": "44dbb26e-ac9c-41f0-e0cd-1c1d929df0af"
      },
      "source": [
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.2,\n",
        "    patience=3,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model_1.fit(\n",
        "    [preprep_data['text_train'], preprep_data['headline_train_input']],\n",
        "    preprep_data['headline_train_output'],\n",
        "    batch_size=500,\n",
        "    epochs=90,\n",
        "    validation_data=([preprep_data['text_test'], preprep_data['headline_test_input']], preprep_data['headline_test_output']),callbacks=[reduce_lr])\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/90\n",
            "186/186 [==============================] - 59s 319ms/step - loss: 5.5691 - val_loss: 4.9990\n",
            "Epoch 2/90\n",
            "186/186 [==============================] - 59s 315ms/step - loss: 4.8741 - val_loss: 4.5400\n",
            "Epoch 3/90\n",
            "186/186 [==============================] - 59s 315ms/step - loss: 4.4664 - val_loss: 4.2610\n",
            "Epoch 4/90\n",
            "186/186 [==============================] - 59s 315ms/step - loss: 4.2025 - val_loss: 4.0786\n",
            "Epoch 5/90\n",
            "186/186 [==============================] - 59s 315ms/step - loss: 4.0063 - val_loss: 3.9498\n",
            "Epoch 6/90\n",
            "186/186 [==============================] - 59s 315ms/step - loss: 3.8504 - val_loss: 3.8294\n",
            "Epoch 7/90\n",
            "186/186 [==============================] - 59s 315ms/step - loss: 3.7237 - val_loss: 3.7631\n",
            "Epoch 8/90\n",
            "186/186 [==============================] - 59s 315ms/step - loss: 3.6175 - val_loss: 3.6904\n",
            "Epoch 9/90\n",
            "186/186 [==============================] - 59s 315ms/step - loss: 3.5274 - val_loss: 3.6355\n",
            "Epoch 10/90\n",
            "186/186 [==============================] - 59s 315ms/step - loss: 3.4521 - val_loss: 3.5941\n",
            "Epoch 11/90\n",
            "186/186 [==============================] - 59s 315ms/step - loss: 3.3858 - val_loss: 3.5534\n",
            "Epoch 12/90\n",
            "186/186 [==============================] - 59s 315ms/step - loss: 3.3274 - val_loss: 3.5184\n",
            "Epoch 13/90\n",
            "186/186 [==============================] - 59s 315ms/step - loss: 3.2744 - val_loss: 3.5164\n",
            "Epoch 14/90\n",
            "186/186 [==============================] - 59s 315ms/step - loss: 3.2294 - val_loss: 3.4857\n",
            "Epoch 15/90\n",
            "186/186 [==============================] - 59s 315ms/step - loss: 3.1874 - val_loss: 3.4558\n",
            "Epoch 16/90\n",
            "186/186 [==============================] - 59s 315ms/step - loss: 3.1506 - val_loss: 3.4481\n",
            "Epoch 17/90\n",
            "186/186 [==============================] - 59s 315ms/step - loss: 3.1181 - val_loss: 3.4434\n",
            "Epoch 18/90\n",
            "186/186 [==============================] - 59s 315ms/step - loss: 3.0892 - val_loss: 3.4254\n",
            "Epoch 19/90\n",
            "186/186 [==============================] - 59s 316ms/step - loss: 3.0612 - val_loss: 3.4213\n",
            "Epoch 20/90\n",
            "186/186 [==============================] - 59s 316ms/step - loss: 3.0363 - val_loss: 3.4182\n",
            "Epoch 21/90\n",
            "186/186 [==============================] - 59s 316ms/step - loss: 3.0137 - val_loss: 3.4024\n",
            "Epoch 22/90\n",
            "186/186 [==============================] - 59s 316ms/step - loss: 2.9919 - val_loss: 3.4000\n",
            "Epoch 23/90\n",
            "186/186 [==============================] - 59s 316ms/step - loss: 2.9715 - val_loss: 3.3992\n",
            "Epoch 24/90\n",
            "186/186 [==============================] - 59s 316ms/step - loss: 2.9526 - val_loss: 3.3963\n",
            "Epoch 25/90\n",
            "186/186 [==============================] - 59s 316ms/step - loss: 2.9346 - val_loss: 3.3953\n",
            "Epoch 26/90\n",
            "186/186 [==============================] - 59s 316ms/step - loss: 2.9172 - val_loss: 3.3894\n",
            "Epoch 27/90\n",
            "186/186 [==============================] - 59s 316ms/step - loss: 2.9001 - val_loss: 3.3897\n",
            "Epoch 28/90\n",
            "186/186 [==============================] - 59s 316ms/step - loss: 2.8839 - val_loss: 3.3861\n",
            "Epoch 29/90\n",
            "186/186 [==============================] - 59s 316ms/step - loss: 2.8690 - val_loss: 3.3840\n",
            "Epoch 30/90\n",
            "186/186 [==============================] - 59s 318ms/step - loss: 2.8570 - val_loss: 3.3791\n",
            "Epoch 31/90\n",
            "186/186 [==============================] - 59s 317ms/step - loss: 2.8461 - val_loss: 3.3781\n",
            "Epoch 32/90\n",
            "186/186 [==============================] - 59s 316ms/step - loss: 2.8359 - val_loss: 3.3724\n",
            "Epoch 33/90\n",
            "186/186 [==============================] - 59s 316ms/step - loss: 2.8262 - val_loss: 3.3703\n",
            "Epoch 34/90\n",
            "186/186 [==============================] - 59s 316ms/step - loss: 2.8156 - val_loss: 3.3913\n",
            "Epoch 35/90\n",
            "186/186 [==============================] - 59s 316ms/step - loss: 2.8051 - val_loss: 3.3932\n",
            "Epoch 36/90\n",
            "186/186 [==============================] - 59s 316ms/step - loss: 2.7948 - val_loss: 3.3672\n",
            "Epoch 37/90\n",
            "186/186 [==============================] - 59s 317ms/step - loss: 2.7846 - val_loss: 3.3754\n",
            "Epoch 38/90\n",
            "186/186 [==============================] - 59s 316ms/step - loss: 2.7764 - val_loss: 3.3733\n",
            "Epoch 39/90\n",
            "186/186 [==============================] - ETA: 0s - loss: 2.7678\n",
            "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n",
            "186/186 [==============================] - 59s 317ms/step - loss: 2.7678 - val_loss: 3.3737\n",
            "Epoch 40/90\n",
            "186/186 [==============================] - 59s 316ms/step - loss: 2.6828 - val_loss: 3.3540\n",
            "Epoch 41/90\n",
            "186/186 [==============================] - 59s 316ms/step - loss: 2.6753 - val_loss: 3.3535\n",
            "Epoch 42/90\n",
            "186/186 [==============================] - 59s 316ms/step - loss: 2.6726 - val_loss: 3.3555\n",
            "Epoch 43/90\n",
            "186/186 [==============================] - 59s 316ms/step - loss: 2.6702 - val_loss: 3.3588\n",
            "Epoch 44/90\n",
            "186/186 [==============================] - ETA: 0s - loss: 2.6683\n",
            "Epoch 00044: ReduceLROnPlateau reducing learning rate to 8.000000379979611e-05.\n",
            "186/186 [==============================] - 59s 316ms/step - loss: 2.6683 - val_loss: 3.3587\n",
            "Epoch 45/90\n",
            "186/186 [==============================] - 59s 317ms/step - loss: 2.6499 - val_loss: 3.3571\n",
            "Epoch 46/90\n",
            "186/186 [==============================] - 59s 317ms/step - loss: 2.6489 - val_loss: 3.3580\n",
            "Epoch 47/90\n",
            "186/186 [==============================] - ETA: 0s - loss: 2.6484\n",
            "Epoch 00047: ReduceLROnPlateau reducing learning rate to 1.6000001050997525e-05.\n",
            "186/186 [==============================] - 59s 317ms/step - loss: 2.6484 - val_loss: 3.3584\n",
            "Epoch 48/90\n",
            "186/186 [==============================] - 59s 317ms/step - loss: 2.6444 - val_loss: 3.3586\n",
            "Epoch 49/90\n",
            "186/186 [==============================] - 59s 317ms/step - loss: 2.6442 - val_loss: 3.3586\n",
            "Epoch 50/90\n",
            "186/186 [==============================] - ETA: 0s - loss: 2.6441\n",
            "Epoch 00050: ReduceLROnPlateau reducing learning rate to 3.2000003557186575e-06.\n",
            "186/186 [==============================] - 59s 317ms/step - loss: 2.6441 - val_loss: 3.3588\n",
            "Epoch 51/90\n",
            "186/186 [==============================] - 59s 316ms/step - loss: 2.6433 - val_loss: 3.3588\n",
            "Epoch 52/90\n",
            "186/186 [==============================] - 59s 316ms/step - loss: 2.6432 - val_loss: 3.3588\n",
            "Epoch 53/90\n",
            "186/186 [==============================] - ETA: 0s - loss: 2.6432\n",
            "Epoch 00053: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-07.\n",
            "186/186 [==============================] - 59s 317ms/step - loss: 2.6432 - val_loss: 3.3588\n",
            "Epoch 54/90\n",
            "186/186 [==============================] - 59s 316ms/step - loss: 2.6430 - val_loss: 3.3588\n",
            "Epoch 55/90\n",
            "186/186 [==============================] - 59s 317ms/step - loss: 2.6430 - val_loss: 3.3588\n",
            "Epoch 56/90\n",
            "186/186 [==============================] - ETA: 0s - loss: 2.6430\n",
            "Epoch 00056: ReduceLROnPlateau reducing learning rate to 1.280000105907675e-07.\n",
            "186/186 [==============================] - 59s 317ms/step - loss: 2.6430 - val_loss: 3.3588\n",
            "Epoch 57/90\n",
            "186/186 [==============================] - 59s 317ms/step - loss: 2.6430 - val_loss: 3.3588\n",
            "Epoch 58/90\n",
            "186/186 [==============================] - 59s 317ms/step - loss: 2.6430 - val_loss: 3.3588\n",
            "Epoch 59/90\n",
            "186/186 [==============================] - ETA: 0s - loss: 2.6430\n",
            "Epoch 00059: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-08.\n",
            "186/186 [==============================] - 59s 317ms/step - loss: 2.6430 - val_loss: 3.3588\n",
            "Epoch 60/90\n",
            "186/186 [==============================] - 59s 316ms/step - loss: 2.6430 - val_loss: 3.3588\n",
            "Epoch 61/90\n",
            "186/186 [==============================] - 59s 317ms/step - loss: 2.6430 - val_loss: 3.3588\n",
            "Epoch 62/90\n",
            "186/186 [==============================] - ETA: 0s - loss: 2.6430\n",
            "Epoch 00062: ReduceLROnPlateau reducing learning rate to 5.1200004236306995e-09.\n",
            "186/186 [==============================] - 59s 317ms/step - loss: 2.6430 - val_loss: 3.3588\n",
            "Epoch 63/90\n",
            "186/186 [==============================] - 59s 316ms/step - loss: 2.6430 - val_loss: 3.3588\n",
            "Epoch 64/90\n",
            "186/186 [==============================] - 59s 318ms/step - loss: 2.6430 - val_loss: 3.3588\n",
            "Epoch 65/90\n",
            "186/186 [==============================] - ETA: 0s - loss: 2.6430\n",
            "Epoch 00065: ReduceLROnPlateau reducing learning rate to 1.02400008472614e-09.\n",
            "186/186 [==============================] - 59s 316ms/step - loss: 2.6430 - val_loss: 3.3588\n",
            "Epoch 66/90\n",
            "186/186 [==============================] - 59s 317ms/step - loss: 2.6430 - val_loss: 3.3588\n",
            "Epoch 67/90\n",
            "186/186 [==============================] - 59s 317ms/step - loss: 2.6430 - val_loss: 3.3588\n",
            "Epoch 68/90\n",
            "186/186 [==============================] - ETA: 0s - loss: 2.6430\n",
            "Epoch 00068: ReduceLROnPlateau reducing learning rate to 2.048000213861201e-10.\n",
            "186/186 [==============================] - 59s 316ms/step - loss: 2.6430 - val_loss: 3.3588\n",
            "Epoch 69/90\n",
            "186/186 [==============================] - 59s 316ms/step - loss: 2.6430 - val_loss: 3.3588\n",
            "Epoch 70/90\n",
            "186/186 [==============================] - 59s 316ms/step - loss: 2.6430 - val_loss: 3.3588\n",
            "Epoch 71/90\n",
            "186/186 [==============================] - ETA: 0s - loss: 2.6430\n",
            "Epoch 00071: ReduceLROnPlateau reducing learning rate to 4.0960004832335533e-11.\n",
            "186/186 [==============================] - 59s 317ms/step - loss: 2.6430 - val_loss: 3.3588\n",
            "Epoch 72/90\n",
            "186/186 [==============================] - 59s 316ms/step - loss: 2.6430 - val_loss: 3.3588\n",
            "Epoch 73/90\n",
            "186/186 [==============================] - 59s 317ms/step - loss: 2.6430 - val_loss: 3.3588\n",
            "Epoch 74/90\n",
            "186/186 [==============================] - ETA: 0s - loss: 2.6430\n",
            "Epoch 00074: ReduceLROnPlateau reducing learning rate to 8.192001244022862e-12.\n",
            "186/186 [==============================] - 59s 317ms/step - loss: 2.6430 - val_loss: 3.3588\n",
            "Epoch 75/90\n",
            "186/186 [==============================] - 59s 317ms/step - loss: 2.6430 - val_loss: 3.3588\n",
            "Epoch 76/90\n",
            "186/186 [==============================] - 59s 317ms/step - loss: 2.6430 - val_loss: 3.3588\n",
            "Epoch 77/90\n",
            "186/186 [==============================] - ETA: 0s - loss: 2.6430\n",
            "Epoch 00077: ReduceLROnPlateau reducing learning rate to 1.6384001794156334e-12.\n",
            "186/186 [==============================] - 59s 317ms/step - loss: 2.6430 - val_loss: 3.3588\n",
            "Epoch 78/90\n",
            "186/186 [==============================] - 59s 317ms/step - loss: 2.6430 - val_loss: 3.3588\n",
            "Epoch 79/90\n",
            "186/186 [==============================] - 59s 317ms/step - loss: 2.6430 - val_loss: 3.3588\n",
            "Epoch 80/90\n",
            "186/186 [==============================] - ETA: 0s - loss: 2.6430\n",
            "Epoch 00080: ReduceLROnPlateau reducing learning rate to 3.276800272095093e-13.\n",
            "186/186 [==============================] - 59s 317ms/step - loss: 2.6430 - val_loss: 3.3588\n",
            "Epoch 81/90\n",
            "186/186 [==============================] - 59s 317ms/step - loss: 2.6430 - val_loss: 3.3588\n",
            "Epoch 82/90\n",
            "186/186 [==============================] - 59s 318ms/step - loss: 2.6430 - val_loss: 3.3588\n",
            "Epoch 83/90\n",
            "186/186 [==============================] - ETA: 0s - loss: 2.6430\n",
            "Epoch 00083: ReduceLROnPlateau reducing learning rate to 6.553600435769969e-14.\n",
            "186/186 [==============================] - 59s 317ms/step - loss: 2.6430 - val_loss: 3.3588\n",
            "Epoch 84/90\n",
            "186/186 [==============================] - 59s 317ms/step - loss: 2.6430 - val_loss: 3.3588\n",
            "Epoch 85/90\n",
            "186/186 [==============================] - 59s 317ms/step - loss: 2.6430 - val_loss: 3.3588\n",
            "Epoch 86/90\n",
            "186/186 [==============================] - ETA: 0s - loss: 2.6430\n",
            "Epoch 00086: ReduceLROnPlateau reducing learning rate to 1.3107200600489395e-14.\n",
            "186/186 [==============================] - 59s 317ms/step - loss: 2.6430 - val_loss: 3.3588\n",
            "Epoch 87/90\n",
            "186/186 [==============================] - 59s 317ms/step - loss: 2.6430 - val_loss: 3.3588\n",
            "Epoch 88/90\n",
            "186/186 [==============================] - 59s 317ms/step - loss: 2.6430 - val_loss: 3.3588\n",
            "Epoch 89/90\n",
            "186/186 [==============================] - ETA: 0s - loss: 2.6430\n",
            "Epoch 00089: ReduceLROnPlateau reducing learning rate to 2.621440086216561e-15.\n",
            "186/186 [==============================] - 59s 318ms/step - loss: 2.6430 - val_loss: 3.3588\n",
            "Epoch 90/90\n",
            "186/186 [==============================] - 59s 317ms/step - loss: 2.6430 - val_loss: 3.3588\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HHxE8WSpL6a",
        "outputId": "1cc9ca60-7dde-4227-ad0d-01a5531c6dab"
      },
      "source": [
        "for idx, layer in enumerate(model_1.layers):\n",
        "    print(f'{idx} => {layer.name}')\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 => encoder\n",
            "1 => decoder\n",
            "2 => decoder_dense\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6qY0caWi90r"
      },
      "source": [
        "index_to_word_text, index_to_word_headline  = {}, {}\n",
        "\n",
        "for key, val in preprep_data['text_tokenizer'].word_index.items():\n",
        "    index_to_word_text[val] = key\n",
        "\n",
        "for key, val in preprep_data['headline_tokenizer'].word_index.items():\n",
        "    index_to_word_headline[val] = key\n",
        "    \n",
        "index_to_word_text[0] = '<pad>'\n",
        "index_to_word_headline[0] = '<pad>'\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLnanfl1i9vD",
        "outputId": "d320f50d-62d3-4e9e-acca-b19977533162"
      },
      "source": [
        "sample_text = preprep_data['text_test'][0]\n",
        "sample_text\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  357,    71,  1541,  3742,  4041,  2650,  7187,  1904,  1150,\n",
              "       11601,  3142,     3,     1,   476,    44,  2696,   101,    28,\n",
              "         707,   176,  5926,  4495,  1275,     6,     1,   786,   104,\n",
              "           4,     1,   786,   144,     6,   162,     1,   240,  1158,\n",
              "          69,  4041,   424,     3,     2,  5352,   163,  1150, 12736,\n",
              "         701,    27,  1052,  4495,     5, 12776,   338,  3142,    27,\n",
              "          11,  2230,   714,     1,   163,  1150,     0,     0,     0],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO0ejigii9qS"
      },
      "source": [
        "func_text = np.vectorize(lambda x: index_to_word_text[x])\n",
        "func_headline = np.vectorize(lambda x: index_to_word_headline[x])\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzSSilw2i9ms",
        "outputId": "ee5e90f2-bfbc-4abb-b06f-211fa3923b56"
      },
      "source": [
        "func_text(sample_text)\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['england', 'all', 'rounder', 'ben', 'stokes', 'accidentally',\n",
              "       'punched', 'leg', 'spinner', 'adil', 'rashid', 'in', 'the', 'face',\n",
              "       'while', 'celebrating', 'team', 'india', 'vice', 'captain',\n",
              "       'ajinkya', 'rahane', 'wicket', 'on', 'the', 'fourth', 'day', 'of',\n",
              "       'the', 'fourth', 'test', 'on', 'sunday', 'the', 'incident',\n",
              "       'happened', 'when', 'stokes', 'went', 'in', 'to', 'hug', 'off',\n",
              "       'spinner', 'moeen', 'ali', 'who', 'dismissed', 'rahane', 'and',\n",
              "       'inadvertently', 'hit', 'rashid', 'who', 'was', 'standing',\n",
              "       'behind', 'the', 'off', 'spinner', '<pad>', '<pad>', '<pad>'],\n",
              "      dtype='<U13')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOFxvVAjvgl-",
        "outputId": "976024b5-d8a2-4bdd-e6fd-7b0ed77cd423"
      },
      "source": [
        "sample_decoder_input = preprep_data['headline_test_input'][0]\n",
        "sample_decoder_input\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   1, 2559, 2123, 5453, 2260,  295, 4241, 1345,    0,    0,    0,\n",
              "          0,    0,    0,    0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcRpA7zMvgcX",
        "outputId": "df66c807-c103-4807-d725-ebad3a9546cd"
      },
      "source": [
        "func_headline(sample_decoder_input)\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['start', 'stokes', 'mistakenly', 'punches', 'rashid', 'while',\n",
              "       'celebrating', 'wicket', '<pad>', '<pad>', '<pad>', '<pad>',\n",
              "       '<pad>', '<pad>', '<pad>'], dtype='<U11')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42zGxxtgv0oW"
      },
      "source": [
        "## Running text to see the predicted summary and actual summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-ZGTupNvgYv"
      },
      "source": [
        "def summarize(long_text):\n",
        "    long_text = long_text.reshape(1, -1)\n",
        "    out, hidden_state, cell_state = model_1.get_layer('encoder')(long_text)\n",
        "    summary = []\n",
        "    \n",
        "    output = np.array([1]).reshape(1, -1)\n",
        "    while True:\n",
        "        output, hidden_state, cell_state = model_1.get_layer('decoder')(output, hidden_state, cell_state)\n",
        "        output = np.argmax(model_1.get_layer('decoder_dense')(output), axis=-1)\n",
        "        pred = index_to_word_headline[int(output)]\n",
        "    \n",
        "        if pred == '<end>' or len(summary) >= MAX_HEADLINE_SEQ_LEN:\n",
        "            break\n",
        "        \n",
        "        summary.append(pred)\n",
        "\n",
        "    return ' '.join(summary)\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oD_QMjoxM77",
        "outputId": "37471920-1ada-48c6-a6be-e297956125bb"
      },
      "source": [
        "for idx in range(100, 200, 10):\n",
        "    predicted = summarize(preprep_data['text_test'][idx])\n",
        "    actual = ' '.join([word for word in func_headline(preprep_data['headline_test_output'][idx]) if word not in ['<pad>', '<end>']])\n",
        "\n",
        "    print(f'GENERATED: {predicted}\\n\\nACTUAL: {actual}\\n\\nACTUAL TEXT: {\" \".join([word for word in func_text(preprep_data[\"text_test\"][idx]) if word not in [\"<pad>\", \"<end>\"]])}\\n')\n",
        "    print('='*128)\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GENERATED: gayle files defamation suit against fixing charges of team end end end end end end\n",
            "\n",
            "ACTUAL: chris gayle files 1m defamation suit over exposing claims end\n",
            "\n",
            "ACTUAL TEXT: windies all rounder chris gayle has filed 1 million defamation suit against three australian newspapers that have accused him of exposing to masseuse during the 2015 world cup the newspapers claimed gayle intentionally exposed his genitals to woman in the dressing room during training session gayle will provide evidence in the nsw supreme court later this month\n",
            "\n",
            "================================================================================================================================\n",
            "GENERATED: bjp leader who commits suicide after being dies in bihar end end end end end\n",
            "\n",
            "ACTUAL: bihar mp 24 yr old son dies as car hits divider on expressway end\n",
            "\n",
            "ACTUAL TEXT: lok janshakti party ljp mp from bihar veena devi elder son ashutosh singh aged 24 died on saturday after his toyota car hit the divider on greater noida expressway and turned upside down he was rushed to hospital where he was declared dead his father singh is also former mp from bihar\n",
            "\n",
            "================================================================================================================================\n",
            "GENERATED: sidhu seeks ã¢ââ¹1 lakh in haryana amid diplomatic crisis end end end end end end\n",
            "\n",
            "ACTUAL: sidhu pays ã¢ââ¹15l compensation from his own pocket to farmers end\n",
            "\n",
            "ACTUAL TEXT: punjab minister navjot singh sidhu on wednesday paid compensation of ã¢ââ¹15 lakh each to farmers whose crops were damaged in fire that broke out in the fields of few villages in amritsar earlier in april when the crops were damaged sidhu had announced that he would help the farmers as much as possible\n",
            "\n",
            "================================================================================================================================\n",
            "GENERATED: bjp mp asks to give up to give up to students in mp end end\n",
            "\n",
            "ACTUAL: pupils made to vow not to vote bjp till it ends online exams end\n",
            "\n",
            "ACTUAL TEXT: students of shri industrial training institute in madhya pradesh were asked to take pledge not to vote for bjp until the government stopped conducting online exams video showed students saying will encourage at least citizens to take such pledge will spread awareness about the corruption and injustice of bjp in my village\n",
            "\n",
            "================================================================================================================================\n",
            "GENERATED: player to play for world cup for winter olympics end end end end end end\n",
            "\n",
            "ACTUAL: record number of condoms to be given at 2018 winter olympics end\n",
            "\n",
            "ACTUAL TEXT: with 110 000 condoms set to be made available during the event 2018 winter olympics in pyeongchang will set record for the most number of condoms to be handed out at winter olympics with total of 925 athletes set to take part in pyeongchang it to an average of 37 6 condoms per sportsman and sportswoman\n",
            "\n",
            "================================================================================================================================\n",
            "GENERATED: twitter reacts to death after wife kim summit end end end end end end end\n",
            "\n",
            "ACTUAL: childhood find each other after 12 yrs through twitter end\n",
            "\n",
            "ACTUAL TEXT: america brianna cry reunited with her childhood friend heidi tran after 12 years through twitter hey twitter met this girl on dinner cruise in hawaii in 2006 need all to help me find my brianna tweeted and posted picture of them heard you were looking for me brianna friend replied to her within 12 hours\n",
            "\n",
            "================================================================================================================================\n",
            "GENERATED: hizbul chief chief army officer for killing indian soldiers end end end end end end\n",
            "\n",
            "ACTUAL: hizbul blames indian agencies for killing army officer fayaz end\n",
            "\n",
            "ACTUAL TEXT: hizbul mujahideen chief syed salahuddin has blamed the indian agencies for killing indian army officer ummer fayaz who according to him belonged to pro freedom family salahuddin has also denied any hand in the abduction and killing of fayaz on the other hand the jammu and kashmir police has released the pictures of three suspected hizbul militants in fayaz killing\n",
            "\n",
            "================================================================================================================================\n",
            "GENERATED: isro to launch india first time in space in space end end end end end\n",
            "\n",
            "ACTUAL: indian astronaut to be in space for days at under ã¢ââ¹10 000 cr isro end\n",
            "\n",
            "ACTUAL TEXT: isro is planning to put an indian astronaut in space in its first manned mission by 2022 for at least seven days at cost of less than ã¢ââ¹10 000 crore isro chairman sivan said on wednesday isro has already tested the crew module and crew escape systems the iaf will select the astronaut who will be trained overseas\n",
            "\n",
            "================================================================================================================================\n",
            "GENERATED: i don know why he is indians by indians in india end day end end\n",
            "\n",
            "ACTUAL: go hell with what people have to say ravi shastri on critics end\n",
            "\n",
            "ACTUAL TEXT: india coach ravi shastri who was criticised after india lost the test series against england said he doesn have time for what people say about him go hell with what people have to say as long as you re confident of what you re doing he added if you start worrying you start messing up so say ignorance is bliss he further said\n",
            "\n",
            "================================================================================================================================\n",
            "GENERATED: over 100 terrorists to be in saudi arabia arabia end day end end end end\n",
            "\n",
            "ACTUAL: 500 sikh pilgrims to visit kartarpur per day pak draft agreement end\n",
            "\n",
            "ACTUAL TEXT: only 500 sikh pilgrims will be allowed per day through the kartarpur corridor and the pilgrims must constitute group of 15 people according to draft agreement by pakistan the draft agreement leaked to the pakistan media proposes visa free trip for the pilgrims but they will have to carry indian passports and security clearance from india\n",
            "\n",
            "================================================================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGUfw8mfxM5n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VqdfU1bxM1_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}